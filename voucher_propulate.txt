Abstract:
Strong progress in machine learning (ML), particularly deep learning, has triggered a widespread use of these techniques in science and industry. ML algorithms typically have hyperparameters (HPs), i.e., non-learnable parameters controlling the training process that can be tuned to improve empirical performance. Only recently, automated approaches to solving such algorithm configuration problems (AutoML) have facilitated the application of ML significantly. Among these, so-called genetic algorithms (GAs) are a class of metaheuristic optimizers inspired by Darwin's theory of natural selection. To find good solutions to a "search for solutions" problem, a multi-set of candidate solutions is evolved towards better solutions in a repeated interplay of evaluation, selection and variation. In this way, GAs can sample large search spaces efficiently in a "survival of the fittest" style. Even though there is no guarantee for reaching the global optimum, they often find near-optimal solutions with less computational effort than classical optimizers and thus have become very popular for solving various optimization problems.
Considering recent advances in parallel computing and the clear trend towards bigger models and datasets, designing scalable algorithms for distributed computing environments is more than ever important. A popular approach for parallelizing GAs is the so-called island model (IM), where the evolutionary process itself is parallelized by distributing the population over the available compute nodes. Evolution happens in a spatially structured network of islands, which mainly evolve independently but occasionally exchange candidate solutions to coordinate the search on a global level. Parallel GAs have been shown to not only speed up computation but also increase solution diversity and quality. In practice, many parallel implementations are synchronized, i.e., all candidate solutions of each island are evaluated in parallel and updated synchronously for the next iteration. For synchronous IMs, the islands simultaneously exchange candidate solutions after fixed migration intervals, with no computation happening during that time. Technically, such algorithms can be easily monitored in a distributed computing environment. However, synchronizing the evolutionary process requires a centralized control of the algorithm's flow which in turn affects its scalability. Since the compute times for evaluation vary, global synchronization leads to idle times and thus suboptimal use efficiency of parallel architectures. In addition, the speed-up on heterogeneous hardware is inevitably impaired. 
To resolve these problems, we aim at designing a novel massively parallel and fully asynchronous GA based on the island model. Starting from an already existing conceptual draft, we plan to further develop and efficiently parallelize the algorithm in Python via the message passing interface so as to leverage it for practical application on high-performance-computing systems in the form of a ready-to-use Python package. In this way, we hope to optimize the use efficiency of parallel hardware by minimizing idle times in distributed computing environments for the many use cases of HP optimization and neural architecture search. 

\fancyname maintains a continuous population of already evaluated individuals with a softened notion of the typically strictly separated, discrete generations. 
Our contributions include:
    \item A novel PGA based on a fully asynchronized island model with independently processing workers. 
    \item Massive parallelism by asynchronous propagation of continuous populations and migration via efficient communication using the message passing interface.
    \item 
As a proof of concept, we apply \fancyname to the optimization of \begin{enumerate*}[label=(\roman*)]
    \item various benchmarking functions and \item the hyperparameters of a neural network

Abstract:
In the field of photovoltaics, perovskites have emerged as a promising candidate for future commercial solar cells. In order to quantify the quality of the fabricated perovskite layers, deep neural networks can be employed for classification based on the 4-channel image time series captured with a camera-based characterization setup. In the long run, the goal is to adapt and adjust the developed models and install them as condition monitoring systems for the characterization setup, in order to perform real-time supervision and potential modification of the coating process, hence enabling production of perovskite films of better quality and therefore leading to photovoltaics with reproducibly higher power conversion efficiencies (PCEs).

In the first voucher, the focus was laid on initial assessment and exploration of the data, in order to outline the most suitable modelling approach. Results of the voucher included the creation of a curated data set in HDF5 format combined with an analysis of the data prepossessing steps with respect to potential improvements for the experimental setup for future data acquisition.
Upon completion of image preprocessing and conversion, we trained a convolutional neural network based off of the well-known VGG architecture, to classify the coated cells into different PCE value categories, using static frames of the image time series at time points of highest signal intensity. The model predictions yielded promising first results. We further took first steps into the direction of parallel model training.

For a continuation of the voucher we seek to further optimize the initially trained static frame classification model by using advanced frameworks for hyperparameter optimization. Furthermore, we aim to develop an advanced model for incorporation of the temporal components of the dataset, under the consideration of the low sample size vs large feature dimensionality. In that regard, we will use methods of representation learning, together with neural networks for time series analysis.
Due to high feature dimensionality of the individual time frame images and the resulting large data size, this approach will require the utilization of distributed computing methods for data loading and model training.

Abstract: 
Remote sensing imaging systems are considered crucial tools for applications such as Earth observation, lithological mapping, and change detection. Various imaging systems reveal different attributes of a target scene. Among them, optical imaging systems (particularly hyperspectral imaging) have received significant attention. Hyperspectral imaging provides detailed spectral information that can be used for a variety of applications such as mineral mapping and raw material identification, which are the backbone activities at HZDR-HIF. Imaging systems induce, however, different noise types and artifacts into the observed image. For instance, hyperspectral push-broom imaging adds a pattern noise called striping. Noise sources and artifacts reduce the quality of remotely sensed data considerably which affects the subsequent processing steps such as mapping and identification. Image denoising is the task of recovering the true unknown image from the degraded observed image. 
Despite the considerable advances in hyperspectral denoising, there is no (open-source) python-based denoising toolbox provided for hyperspectral images. In this project, we aim at providing a toolbox to fulfill the needs of HIF to acquire data with higher qualities. We will, therefore, select the state-of-the-art techniques in hyperspectral image denoising and provide a python implementation for them.  Additionally, we will parallelize the existing codes and algorithms and provide GPU implementation (preferably with the Pytorch platform) of the existing methods. All the methods that we consider in this project have open source codes implemented in other programming languages and environments such as C and Matlab. It should be noted that most of the approaches investigated in this proposal have been proposed and published by the applicants.
